---
layout: summary
title: Summary
paper: {{zeiler2012adadelta}}
author:  sritank
score:  10
---

* What is the core idea?
A new linear transformer model is introduced that uses a kernel based self-attention method that reduces the memory footprint of the model.

* How is it realized (technically)?

* How well does the paper perform?

* Main idea


* Technical implementation

* Algorithm performance




## TL;DR

