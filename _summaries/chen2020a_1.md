---
layout: summary
title: Summary
paper: {chen2020a_1}
# Please fill out info below
author: specfazhou
score: 8/10
---

TODO: Summarize the paper:
* What is the core idea? <br/>
This paper comes up with a simple framework for contrasive learning of visual representation, which not only archieves state of the art accuracy, but also has simplier structure. The result of SimCLR is shown in the following picture <br/>
<img src = "chen2020_a_3.png">

* How is it realized (technically)? <br/>
The architecture of the SimCLR is shown as the following picture <br/>
<img src = "chen2020_a_1.png">
As you can see from the picture. First, for each data sample, x, the model generates two views of of the data sample by sequentially applying three data augmentation methods -- random cropping, random color distortions, and random Gauessian blur. Then the model applies encoder $$f(-)$$. There are various choices of the encoder, but in this paper, the authors use ResNet. Projection head $$g(-)$$ is MLP with one hidden layer, and it uses ReLU as activation function. The reason we need projection head $$g(-)$$ is that it maps representations to the space where contrasive loss can be applied. <br/>

The training process is shown as following. <br/>
<img src = "chen2020_a_2.png">


* How well does the paper perform?
1. Larger batch size has more advantage than smaller batch size when the number of training epochs is small, but this advantage disappears when the number of epochs increases, as shown in the following picture <br/>
<img src = "chen2020_a_4.png">
2.





* What interesting variants are explored?

## TL;DR
* 
* Bullets
* To highlight the core concepts
