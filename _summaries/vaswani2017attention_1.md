---
layout: summary
title: Summary
paper: vaswani2017attention
# Please fill out info below
author: # samatharhay
score: # 8
---

TODO: Summarize the paper:
* What is the core idea?

The paper introduced an architecture based solely on attention mechanisms, which they call a Transformer, to replace the dominant forms of sequence transduction models, which use recurrence and convolutions.

* How is it realized (technically)?

![Transformer](vaswani2017attention_1a.png)

##Encoder

##Decoder

##Attention

![Attention](vaswani2017attention_1b.png)

##Feedforward

##Embeddings and Softmax

##Positional Encoding

* How well does the paper perform?

##Explination of Hyperparameters in test

##results

![Results](vaswani2017attention_1c.png)


* What interesting variants are explored?



## TL;DR
* Introduced the architecture of a Transformer
* The Transformer model was able to get state of the art BLEU scores
* To highlight the core concepts
