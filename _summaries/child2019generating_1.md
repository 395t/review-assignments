---
layout: summary
title: Summary
paper: child2019generating
# Please fill out info below
author: ishank-arora
score: 9
---

TODO: Summarize the paper:
* What is the core idea?
  * This paper introduces Spare Transformers. 
  * Sparse Transformers introduce sparse factorizations of the attention matrix which reduces the time and memory required by transformers from $$O(n^2)$$ to $$O(n\sqrt{n})$$.
    *  This is realised in three ways
      1. A variation of the architecture and intialisation are used in Sparse Transformers 
      2. Recomputing attention matrices
      3. Fast attention kernels for training
    * The above changes to Transformers is what lead to Sparse Transformers
  * 
* How is it realized (technically)?
* 
* How well does the paper perform?
* What interesting variants are explored?

## TL;DR
* Three
* Bullets
* To highlight the core concepts
