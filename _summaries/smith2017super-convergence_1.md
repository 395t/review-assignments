---
layout: summary
title: Summary
paper: {{smith2017super-convergence}}
# Please fill out info below
author: jchavezberkeley
score: 8
---
* What is the core idea?

This paper examines "super-convergence", the idea that neural networks can be trained faster and achieve higher accuracy rates using higher learning rates, less regularization in other areas, and less training data. The paper also explores methods of computing optimal learning rates using Hessian-free optimization and finally, the effects of different learning rate cycles.

* How is it realized (technically)?


* How well does the paper perform?
* What interesting variants are explored?

## TL;DR
* Three
* Bullets
* To highlight the core concepts
