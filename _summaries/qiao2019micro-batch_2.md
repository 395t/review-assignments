---
layout: summary
title: Summary
paper: {{qiao2019micro-batch_2}}
# Please fill out info below
author: nilesh2797
score: 7
---

TODO: Summarize the paper:
## What is the core idea?
The authors propose and study Weight Standardization (`WS`) and Batch-Channel Normalization (`BCN`) for effectively training deep conv nets on vision tasks. With many empirical results and theoretical analysis they argue the benefits of `WS` and `BCN` for training with small batches.

### Context
Several normalization methods have been proposed for accelerating deep neural net's training. The formulation of normalization is mostly consistent across these normalization methods but differ on the axis of normalization as shown in the figure below ([Source](https://arxiv.org/pdf/1803.08494.pdf))
<img src="qiao2019micro-batch_2/normalization-general.png"/>

## Details
### Weight Standardization (`WS`)

* How well does the paper perform?
* What interesting variants are explored?

## TL;DR
* Three
* Bullets
* To highlight the core concepts
