<!DOCTYPE html>
<html lang="en">
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Summary | cs395T</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Summary" />
<meta name="author" content="elampietti" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The core idea of this paper is to challenge the effectiveness of asymmetric activation functions, such as sigmoid, with the traditional, random initialization for deep architecture. The paper accomplishes this by providing improved results from their normalized initialization process and by using symmetric activation functions such as hyperbolic tangent and softsign." />
<meta property="og:description" content="The core idea of this paper is to challenge the effectiveness of asymmetric activation functions, such as sigmoid, with the traditional, random initialization for deep architecture. The paper accomplishes this by providing improved results from their normalized initialization process and by using symmetric activation functions such as hyperbolic tangent and softsign." />
<meta property="og:site_name" content="cs395T" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-09-08T21:36:34-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Summary" />
<script type="application/ld+json">
{"description":"The core idea of this paper is to challenge the effectiveness of asymmetric activation functions, such as sigmoid, with the traditional, random initialization for deep architecture. The paper accomplishes this by providing improved results from their normalized initialization process and by using symmetric activation functions such as hyperbolic tangent and softsign.","author":{"@type":"Person","name":"elampietti"},"@type":"BlogPosting","url":"/summaries/glorot2010understanding_2.html","headline":"Summary","dateModified":"2021-09-08T21:36:34-05:00","datePublished":"2021-09-08T21:36:34-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"/summaries/glorot2010understanding_2.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/main.css"><link type="application/atom+xml" rel="alternate" href="/feed.xml" title="cs395T" /><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">cs395T</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="subtitle">Invalid paper tag!</div>

<table class="overview">
    <tr><td>author:</td><td> elampietti</td></tr>
    <tr><td>score:</td><td> 8 / 10</td></tr>
</table>

<p>The core idea of this paper is to challenge the effectiveness of asymmetric activation functions, such as sigmoid, with the traditional, random initialization for deep architecture.
The paper accomplishes this by providing improved results from their normalized initialization process and by using symmetric activation functions such as hyperbolic tangent and softsign.</p>

<p>The Shapeset-3 x 2 dataset is used in this paper for the test models to predict 9 possible shape classifications.
Feedforward neural networks with a variation of one to five hidden layers each consisting of one thousand hidden nodes and an output layer of softmax logisitic regression was used for the following experiments.</p>

<p>The paper makes a point that studying the changes in gradients and activation function values between epochs and network layers is informative towards the effectiveness of a model.</p>

<p>With deeper networks, the variance of the gradient will decrease using the standard initialization as you back-propagate through the network and ultimately result in slower learning, therefore they created a normalized initialization to make a more consistent varience in gradient/activation values throughout the network.</p>

<p>Here is the standard initialization that has a multiplicative effect of decreasing back-propagated gradients where ‘n’ is the number of nodes in the layer:</p>

<p><img src="https://user-images.githubusercontent.com/7085644/132159731-98d1bf49-90c7-4873-9c02-a2882d287cd3.PNG" alt="standard" /></p>

<p>Here is the equation for the normalized initialization to mitigate that multiplicative effect:</p>

<p><img src="https://user-images.githubusercontent.com/7085644/132159455-d40e6655-e598-4f12-80af-82ac19a9e5f9.PNG" alt="equation" /></p>

<p>The improvement resulting from the choice of an activation function is realized through experiements tracking the saturation levels of each layer and trying to minimize over-saturation.</p>

<p>The asymmetric activation function, sigmoid, with random initialization showed the highest saturation towards 0 in the top hidden layer compared to the hyperbolic tangent and softsign activation functions which did not encounter this pattern due to their symmetry around 0.
With unsupervised pre-training as initialization, the sigmoid activation function does not produce this high level of saturation.</p>

<p>The hyperbolic tangent activation did experience a saturation pattern from the bottom to the top of the network which needs to be investigated further to explain according to the paper, however it performed better than sigmoid and benefitted the most from the normalized initialization.</p>

<p>The softsign activation had the lowest saturation rate due to its quadratic polynomial tails which allow values to approach its asymptotes at a slower rate.</p>

<p>The paper provides an overall improvement of 9% in the test error from 59.47% to 50.47% with the normalized initialization on a five layer network with the hyperbolic tangent network.</p>

<p>I like how although the paper states that many of their observations are not currently explained, they provide key avenues to further explore these improvements by saying to focus on the varience of gradients and saturation of activation values throughout deep networks.</p>

<h2 id="tldr">TL;DR</h2>
<ul>
  <li>Random initialization combined with a sigmoid activation function is not effective for deep architecture.</li>
  <li>Symmetric activation functions and a normalized initialization is better suited for deep learning.</li>
  <li>Tracking the gradients/activation values between epochs/layers provides insight into the effectiveness of a model.</li>
</ul>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">cs395T</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">cs395T</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p></p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
