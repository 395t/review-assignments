<!DOCTYPE html>
<html lang="en">
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Summary | cs395T</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Summary" />
<meta name="author" content="jchavezberkeley" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This paper proposes the Gaussian Error Linear Unit (GELU), a neural activation function that performs well against ReLU and ELU in similar training experiments. It is often the case that a “deterministic decision” is wanted from a neural network, which motivates the need for a new nonlinearity. It has often been the case that nonlinearities and dropouts were seperate innovations, hence GELUs are nonlinearities that relate to stochastic regularizers by being the “expectation” to changes in dropout. Thus, in other words, a neuron’s output is more probabilistic." />
<meta property="og:description" content="This paper proposes the Gaussian Error Linear Unit (GELU), a neural activation function that performs well against ReLU and ELU in similar training experiments. It is often the case that a “deterministic decision” is wanted from a neural network, which motivates the need for a new nonlinearity. It has often been the case that nonlinearities and dropouts were seperate innovations, hence GELUs are nonlinearities that relate to stochastic regularizers by being the “expectation” to changes in dropout. Thus, in other words, a neuron’s output is more probabilistic." />
<meta property="og:site_name" content="cs395T" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-09-08T21:36:34-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Summary" />
<script type="application/ld+json">
{"description":"This paper proposes the Gaussian Error Linear Unit (GELU), a neural activation function that performs well against ReLU and ELU in similar training experiments. It is often the case that a “deterministic decision” is wanted from a neural network, which motivates the need for a new nonlinearity. It has often been the case that nonlinearities and dropouts were seperate innovations, hence GELUs are nonlinearities that relate to stochastic regularizers by being the “expectation” to changes in dropout. Thus, in other words, a neuron’s output is more probabilistic.","author":{"@type":"Person","name":"jchavezberkeley"},"@type":"BlogPosting","url":"/summaries/hendrycks2016gaussian_2.html","headline":"Summary","dateModified":"2021-09-08T21:36:34-05:00","datePublished":"2021-09-08T21:36:34-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"/summaries/hendrycks2016gaussian_2.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/main.css"><link type="application/atom+xml" rel="alternate" href="/feed.xml" title="cs395T" /><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">cs395T</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="subtitle">Invalid paper tag!</div>

<table class="overview">
    <tr><td>author:</td><td> jchavezberkeley</td></tr>
    <tr><td>score:</td><td> 8 / 10</td></tr>
</table>

<p>This paper proposes the Gaussian Error Linear Unit (GELU), a neural activation function that performs well against ReLU and ELU in similar training experiments. It is often the case that a “deterministic decision” is wanted from a neural network, which motivates the need for a new nonlinearity. It has often been the case that nonlinearities and dropouts were seperate innovations, hence GELUs are nonlinearities that relate to stochastic regularizers by being the “expectation” to changes in dropout. Thus, in other words, a neuron’s output is more probabilistic.</p>

<p>Thus, the GELU is a nonlinearity that weights input by value rather than gates input by sign, such as in ReLUs.</p>

<p>The activiation function is \(x \Phi(x)\).</p>

<p>This essentially scales $x$ by how much greater it is than other inputs.</p>

<p>In more detail, it is \(x \Phi(x) = x P(X \leq x) = x \frac{1}{2}[1 + erf (x / \sqrt(2))]\) which is approximated with \(0.5 x (1 + tanh[\sqrt(2/\pi)(x + 0.044715x^3)])\).</p>

<h3 id="performance">Performance</h3>
<p>Several experiments were done on the MNIST, TIMIT, CIFAR datasets to see effect this nonlinearity is, especially with dropout.</p>

<p>Here is a comparison between MNIST classificaiton results between the GELU, ReLU, and ELU.
<img src="./hendrycks2016gaussian_2_a.png" width="45%" /></p>

<p>Here is the same comparison but with dropout added.
<img src="./hendrycks2016gaussian_2_b.png" width="45%" /></p>

<p>GELU performed well with dropout. It also proved to perform better during TIMIIT Frame Classification.
<img src="./hendrycks2016gaussian_2_c.png" width="45%" /></p>

<p>Finally, during CIFAR classification tests, a more intricate architecture, the GELU nonlinearity proved to outperform ReLUs and ELUs.
<img src="./hendrycks2016gaussian_2_d.png" width="45%" /></p>

<p>In the end, the paper recommends two tips when using GELUs:</p>
<ol>
  <li>Using an optimizer with momentum, standard for deep neural networks</li>
  <li>Use a close approximation to the CDF of a Gaussian distribution</li>
</ol>

<h2 id="tldr">TL;DR</h2>
<ul>
  <li>The GELU is a nonlinearity</li>
  <li>Performs well with dropout</li>
  <li>Outperforms ReLUs and ELUs on harder classification tasks.</li>
</ul>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">cs395T</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">cs395T</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p></p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
