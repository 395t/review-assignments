<!DOCTYPE html>
<html lang="en">
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Summary | cs395T</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Summary" />
<meta name="author" content="philkr" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="If AlexNet started the modern era of deep learning, ResNets made sure it would spread to all of computer vision and beyond. The core idea of residual networks is to parametrize each layer (or group of layers) as a residual of a function. A function \(f_1(x)\) would now become \(f_2(x) = x + g(x)\)." />
<meta property="og:description" content="If AlexNet started the modern era of deep learning, ResNets made sure it would spread to all of computer vision and beyond. The core idea of residual networks is to parametrize each layer (or group of layers) as a residual of a function. A function \(f_1(x)\) would now become \(f_2(x) = x + g(x)\)." />
<meta property="og:site_name" content="cs395T" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-09-08T21:36:34-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Summary" />
<script type="application/ld+json">
{"description":"If AlexNet started the modern era of deep learning, ResNets made sure it would spread to all of computer vision and beyond. The core idea of residual networks is to parametrize each layer (or group of layers) as a residual of a function. A function \\(f_1(x)\\) would now become \\(f_2(x) = x + g(x)\\).","author":{"@type":"Person","name":"philkr"},"@type":"BlogPosting","url":"/summaries/he2015deep_1.html","headline":"Summary","dateModified":"2021-09-08T21:36:34-05:00","datePublished":"2021-09-08T21:36:34-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"/summaries/he2015deep_1.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/main.css"><link type="application/atom+xml" rel="alternate" href="/feed.xml" title="cs395T" /><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">cs395T</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="subtitle">Invalid paper tag!</div>

<table class="overview">
    <tr><td>author:</td><td> philkr</td></tr>
    <tr><td>score:</td><td> 10 / 10</td></tr>
</table>

<p>If AlexNet started the modern era of deep learning, ResNets made sure it would spread to all of computer vision and beyond.
The core idea of residual networks is to parametrize each layer (or group of layers) as a residual of a function.
A function \(f_1(x)\) would now become \(f_2(x) = x + g(x)\).</p>

<p><img width="200px" src="he2015deep_1a.jpg" /></p>

<p>Both representations have the same expressional power and computational requirements.
However, they have a different gradient.
For regular networks, back-propagation multiplies a gradient with \(\frac{\partial}{\partial x} f_1(x)\), while residual networks use \(\frac{\partial}{\partial x} x + g(x) = I + \frac{\partial}{\partial x} g(x)\).
Hence, back-propagation passes the gradient back directly, and adds a second component multiplied by \(\frac{\partial}{\partial x} g(x)\).</p>

<p>Fun fact, the backward pass of a resnet block looks the same as the forward pass</p>

<p><img width="200px" src="he2015deep_1b.jpg" /></p>

<p>How do we build a network out of this idea?
There are two variants of basic-residual network blocks: post-activation blocks (below left), pre-activation blocks (below right).</p>

<p><img width="200px" src="he2015deep_1c.jpg" /></p>

<p>ResNets then stack many of these blocks in a network</p>

<p><img width="100px" src="he2015deep_1d.jpg" /></p>

<p>The first layer if each series of blocks is strided, and itâ€™s residual connection uses a 1x1 convolution (to better deal with the change in number of channels).</p>

<p>ResNets really blew the competition out of the water across many computer vision tasks. 
They are still used today.</p>

<h2 id="tldr">TL;DR</h2>
<ul>
  <li>Introduces <strong>ResNet</strong>: First really deep networks</li>
  <li>Beautiful <strong>mathematical interpretation</strong></li>
  <li><strong>Stellar results</strong></li>
</ul>


      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">cs395T</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">cs395T</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p></p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
