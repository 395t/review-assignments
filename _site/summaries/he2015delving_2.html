<!DOCTYPE html>
<html lang="en">
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Summary | cs395T</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Summary" />
<meta name="author" content="jordi1215" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The paper introduces a new generalization of ReLU, called Parametric Rectified Linear Unit (PReLU), which adaptively learns the parameters of the rectifiers, and improves accuracy at negligible extra computational cost." />
<meta property="og:description" content="The paper introduces a new generalization of ReLU, called Parametric Rectified Linear Unit (PReLU), which adaptively learns the parameters of the rectifiers, and improves accuracy at negligible extra computational cost." />
<meta property="og:site_name" content="cs395T" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-09-08T21:36:34-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Summary" />
<script type="application/ld+json">
{"description":"The paper introduces a new generalization of ReLU, called Parametric Rectified Linear Unit (PReLU), which adaptively learns the parameters of the rectifiers, and improves accuracy at negligible extra computational cost.","author":{"@type":"Person","name":"jordi1215"},"@type":"BlogPosting","url":"/summaries/he2015delving_2.html","headline":"Summary","dateModified":"2021-09-08T21:36:34-05:00","datePublished":"2021-09-08T21:36:34-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"/summaries/he2015delving_2.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/main.css"><link type="application/atom+xml" rel="alternate" href="/feed.xml" title="cs395T" /><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">cs395T</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="subtitle">Invalid paper tag!</div>

<table class="overview">
    <tr><td>author:</td><td> jordi1215</td></tr>
    <tr><td>score:</td><td> 10 / 10</td></tr>
</table>

<p>The paper introduces a new generalization of ReLU, called Parametric Rectified Linear Unit (PReLU), which adaptively learns the parameters of the rectifiers, and improves accuracy at negligible extra computational cost.
<img src="https://user-images.githubusercontent.com/17770319/131951183-e94ad855-480d-4ed0-a5c5-c69b66127b05.png" alt="image" />
<img src="https://user-images.githubusercontent.com/17770319/131951231-54edfeaa-6c8c-4bc1-bc82-67d64cf4efe5.png" alt="image" /></p>

<p>PReLU introduces a very small number of extra parameters. It can be trained using backpropagation and opptimized simultaneously with other layers.</p>

<p>ReLU expedites convergence of the training procedure and leads to better solutions than conventional sigmoid like units.</p>

<p>The paper also studies the difficulty of training rectified models that are very deep. By explicitly modeling the nonlinearity of rectifiers (ReLU/PReLU), the team derives a theoretically sound initialization method, which helps with convergence of very deep models (e.g., with 30 weight layers) trained directly from scratch. This gives more flexibility to explore more powerful network architectures.</p>

<p>Parametric Rectified Linear Unit (PReLU) surpass human-level performance for the classification task in the 1000-class ImageNet dataset.</p>

<h2 id="tldr">TL;DR</h2>
<ul>
  <li>Parametric Rectified Linear Unit (PReLU) with parameter that learns from backpropagation</li>
  <li>ReLU is better than sigmoid</li>
  <li>surpass human-level performance</li>
</ul>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">cs395T</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">cs395T</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p></p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
