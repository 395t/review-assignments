<!DOCTYPE html>
<html lang="en">
<head>
  <meta name="robots" content="noindex">
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Summary | cs395T</title>
<meta name="generator" content="Jekyll v4.2.0" />
<meta property="og:title" content="Summary" />
<meta name="author" content="biofizzatreya" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="TODO: Summarize the paper: This paper attempts to fix the vanishing gradient problem of Relu activation functions by using, what they call Mish. Relu is a conditionally defined function. It has a fixed value 0 for \(x&lt;0\) and is linear for \(x&gt;0\). Since Relu has a fixed value of 0 for \(x&lt;0\) this results in a problem of vanishing gradients, where negative gradients in one layer do not propagate to the next layer. Moreover, Relu itself is not a smooth function as it is condtionally defined at 0. This creates rugged training landscapes. An alternative to using Relu is a function called Swish. Swish is defined as \(x\cdot \text{sigmoid}(\beta\cdot x)\), where the sigmoid function is defined as \(\frac{1}{1+e^{-x}}\). This function makes a smooth-activation function with a shape similar to Relu. This also reduces the vanishing-gradient problem." />
<meta property="og:description" content="TODO: Summarize the paper: This paper attempts to fix the vanishing gradient problem of Relu activation functions by using, what they call Mish. Relu is a conditionally defined function. It has a fixed value 0 for \(x&lt;0\) and is linear for \(x&gt;0\). Since Relu has a fixed value of 0 for \(x&lt;0\) this results in a problem of vanishing gradients, where negative gradients in one layer do not propagate to the next layer. Moreover, Relu itself is not a smooth function as it is condtionally defined at 0. This creates rugged training landscapes. An alternative to using Relu is a function called Swish. Swish is defined as \(x\cdot \text{sigmoid}(\beta\cdot x)\), where the sigmoid function is defined as \(\frac{1}{1+e^{-x}}\). This function makes a smooth-activation function with a shape similar to Relu. This also reduces the vanishing-gradient problem." />
<meta property="og:site_name" content="cs395T" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-09-08T21:36:34-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Summary" />
<script type="application/ld+json">
{"description":"TODO: Summarize the paper: This paper attempts to fix the vanishing gradient problem of Relu activation functions by using, what they call Mish. Relu is a conditionally defined function. It has a fixed value 0 for \\(x&lt;0\\) and is linear for \\(x&gt;0\\). Since Relu has a fixed value of 0 for \\(x&lt;0\\) this results in a problem of vanishing gradients, where negative gradients in one layer do not propagate to the next layer. Moreover, Relu itself is not a smooth function as it is condtionally defined at 0. This creates rugged training landscapes. An alternative to using Relu is a function called Swish. Swish is defined as \\(x\\cdot \\text{sigmoid}(\\beta\\cdot x)\\), where the sigmoid function is defined as \\(\\frac{1}{1+e^{-x}}\\). This function makes a smooth-activation function with a shape similar to Relu. This also reduces the vanishing-gradient problem.","author":{"@type":"Person","name":"biofizzatreya"},"@type":"BlogPosting","url":"/summaries/misra2019mish_2.html","headline":"Summary","dateModified":"2021-09-08T21:36:34-05:00","datePublished":"2021-09-08T21:36:34-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"/summaries/misra2019mish_2.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/main.css"><link type="application/atom+xml" rel="alternate" href="/feed.xml" title="cs395T" /><script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">cs395T</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="subtitle">Invalid paper tag!</div>

<table class="overview">
    <tr><td>author:</td><td> biofizzatreya</td></tr>
    <tr><td>score:</td><td> 7 / 10</td></tr>
</table>

<p>TODO: Summarize the paper:
This paper attempts to fix the vanishing gradient problem of Relu activation functions by using, what they call Mish. 
Relu is a conditionally defined function. It has a fixed value 0 for \(x&lt;0\) and is linear for \(x&gt;0\). Since Relu has a fixed value of 0 for \(x&lt;0\) this results in a problem of vanishing gradients, where negative gradients in one layer do not propagate to the next layer. Moreover, Relu itself is not a smooth function as it is condtionally defined at 0. This creates rugged training landscapes.
An alternative to using Relu is a function called Swish. Swish is defined as \(x\cdot \text{sigmoid}(\beta\cdot x)\), where the sigmoid function is defined as \(\frac{1}{1+e^{-x}}\). This function makes a smooth-activation function with a shape similar to Relu. This also reduces the vanishing-gradient problem.</p>

<p>Mish is influenced by swish and is defined as \(x\cdot tanh\left(\text{ln}(1+e^x)\right)\). The \(\text{ln}(1+e^x)\) is also called a softplus function. The authors compare the effect of using different activation functions such as Relu, Swish and Mish.</p>

<p><img src="https://user-images.githubusercontent.com/13065170/132050780-f3438c1c-a2b6-433d-a3de-2d9279d81cce.png" alt="Picture1" /></p>

<p>Understandably Mish is differentiable and not as rough compared to Relu</p>

<p><img src="https://user-images.githubusercontent.com/13065170/132051192-10e6a969-44fb-42b1-b1b2-ff45573533d1.png" alt="Picture2" /></p>

<p>Mish compares well with other activation functions.</p>

<p><img src="https://user-images.githubusercontent.com/13065170/132051510-42e39ced-5bb2-4589-a94c-fcef71cd8077.png" alt="Picture3" /></p>

<p>Mish seems to work quite well when it comes to overfitting. Compared to other activation functions, Mish does not increase its loss as much as number of layers are increased. Mish also performs more accurately when it comes to training data corrupted by gaussian noise. They have performed other comparisons as well. To distinguish itself from Swish, they show that Swish decreases Top-1 accuracy in larger models, unlike Mish. Mish was also able to seriously reduce the runtime for object detection training, possibly due to its differentiability and lack of conditional statements.</p>

<p><img src="https://user-images.githubusercontent.com/13065170/132052717-1e3aa06d-364c-4d74-b705-a0e057a1e30c.png" alt="Picture4" /></p>

<h2 id="tldr">TL;DR</h2>
<ul>
  <li>A smooth activation function Mish</li>
  <li>Proposed as an alternative to Swish and Relu</li>
  <li>Performs well especially in larger models</li>
</ul>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">cs395T</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">cs395T</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p></p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
