I"≈<ul>
  <li>
    <p>What is the core idea?</p>

    <ul>
      <li>
        <p>First paper (I believe) to prove using rectifier function (\(max(0,  x)\)) better than \(\textit{logistic sigmoid}\) or \(\textit{hyperbolic tangent}\) activation functions. Approaches this from the field of computational neuroscience.</p>
      </li>
      <li>
        <p>Proposes the use of rectifying non-linearities as alternatives to the hyperbolic tangent or sigmoid in deep artificial neural networks, in addition to using an \(L_1\) regularizer on the activation values to promote <strong>sparsity</strong> and prevent potential numerical problems with unbounded activation.</p>
      </li>
      <li>
        <p>Rectifier function brings together the fields of computational neuroscience and machine learning.</p>
      </li>
      <li>
        <p>Rectifying neurons are a better model of biological neurons than hyperbolic tangenet networks.</p>
      </li>
      <li>
        <p>There are gaps between computational neuroscience models and machine learning models. Two main gaps are:</p>

        <ul>
          <li>It‚Äôs estimated that about only 1-4% of neurons are active in the brain at the same time. Ordinary feedforward neural nets (without additional regularization such as an \(L_1\) penalty) do not have this property.
            <ul>
              <li>Ex.: in the steady state, when employing the sigmoid activation function, all neurons fire are activated about 50% of the time. This is biologically implausible and hurts gradient-based optimization.</li>
            </ul>
          </li>
          <li>Computational Neuroscience used Leaky Integrate-and-fire (LIF) activation function. Deep learning and neural networks literature most commonly used tanh and logisitc sigmoid. (see figure 1)</li>
        </ul>

        <p><img src="glorot2011deep_1_1.png" alt="glorot2011deep_1_1" /></p>
      </li>
      <li>
        <p>Advantages of <strong>sparsity</strong>:</p>

        <ul>
          <li>Information disentangling.
            <ul>
              <li>A claimed objective of deep learning algorithms (Bengio, 2009) is to disentangle the factors explainging the variations in the data.</li>
              <li>A dense representation is highly entangled - almost any change in the input modifies most of the entries in the representation vector.</li>
              <li>A sparse representation that is robust to small input changes, therefore, conserves the set of non-zero features.</li>
            </ul>
          </li>
          <li>Efficient variable-size representation
            <ul>
              <li>Different inputs may contain different amounts of information - this can then be reflected with varying amounts of sparsity as a result of the rectifier activation function.</li>
            </ul>
          </li>
          <li>Linear separability
            <ul>
              <li>More sparsity, more likely to be linearly separable simply because data is in represented in a high-dimensional space.</li>
            </ul>
          </li>
          <li>Distributed but sparse
            <ul>
              <li>Information is distributed amongst the non-zero values that if there is some noise and some values have to be discarded, information loss is low.</li>
              <li>Storing is easier, only have to store non-zero values and their locations.</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>
        <p>Disadvantages of sparsity:</p>

        <ul>
          <li>Too much sparsity may hurt ‚Äúpredictive performance for an equal number of neurons‚Äù as it reduces the ‚Äúeffective capaicty of the model.‚Äù</li>
        </ul>
      </li>
    </ul>

    <p><img src="glorot2011deep_1_2.png" alt="glorot2011deep_1_2" /></p>

    <ul>
      <li>Advantages of <strong>rectifier</strong> neurons:
        <ul>
          <li>Allows network to easily obtain sparse representations
            <ul>
              <li>Is more biologicially plausible</li>
            </ul>
          </li>
          <li>Computations are also cheaper as sparsity can be exploited</li>
          <li>No gradient vainishing effect due to activation non-linearities of sigmoid or tanh units.</li>
          <li>Better gradient flow due on active neurons (where computation is linear - see figure 2)</li>
        </ul>
      </li>
      <li>Potential Problems:
        <ul>
          <li>Hard saturation at 0 may hurt optimization
            <ul>
              <li>Smooth version of rectifying non-linearity - \(softplus(x) = \log(1 + e^x)\) (see figure 2)
                <ul>
                  <li>loses exact sparsity but may hope to gain easier training</li>
                </ul>
              </li>
            </ul>
          </li>
          <li>Numerical problems due to unbounded behaviour of the activations
            <ul>
              <li>Use the \(L_1\) penalty on the activation values - also promotes additional sparsity.</li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>How is it realized (technically)?</p>

    <ul>
      <li>Rectifier function is \(max(0,  x)\).</li>
      <li>A smoother version: \(softplus(x) = \log(1 + e^x)\)</li>
    </ul>
  </li>
  <li>
    <p>How well does the paper perform?</p>

    <ul>
      <li>Experiment results
        <ul>
          <li>Sparsity does not hurt performance until around 85% of neurons are 0. (see figure 3)</li>
          <li><img src="glorot2011deep_1_3.png" alt="glorot2011deep_1_3" /></li>
          <li>Rectifiers outperform softplus</li>
          <li>Rectifer outperforms tanh in image recognition.</li>
          <li>No improvement using pre-trained autoencoders - hence just using rectifier activation function is easier to use</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>
    <p>What interesting variants are explored?</p>

    <ul>
      <li>rectifier versus softplus</li>
    </ul>
  </li>
</ul>

<h2 id="tldr">TL;DR</h2>
<ul>
  <li>rectifier activation function better than sigmoid and tanh activation functions</li>
  <li>sparsity is good - increases accuracy, computationally better performance, and representative of the biological neuron</li>
  <li>50-80% sparsity for best generalising models, whereas the brain is hypothesized to have 95% to 99% sparsity.</li>
</ul>
:ET